{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ok6yUicvWHE"
      },
      "source": [
        "# Building a Word Segmentation model for Vietnamese\n",
        "\n",
        "In this assignment, we will build a Word Segmentation model for Vietnamese.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student's name: Ha Trung Tin\n",
        "\n",
        "Student's ID: BI11-261"
      ],
      "metadata": {
        "id": "H6bo2iB2o_kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to submit\n",
        "\n",
        "- Attach notebook file (.ipynb) and submit your work to Google Class Room. **Please do NOT submit URL**\n",
        "- Name your file as YourName_StudentID_Assignment3.ibynb. E.g., Nguyen_Van_A_ST099834_Assignment3.ipynb\n",
        "- Copying others' assignments is strictly prohibited.\n",
        "- Write your name and student ID into this notebook\n",
        "\n",
        "**The due for the programming assignment 3 will be at 23:59 on March 10, 2023 (Hard deadline)**\n",
        "\n",
        "- You will be deducted 5 points for each day late submission\n",
        "- Students who fail to attach the file will not be graded.\n",
        "\n",
        "## Rules\n",
        "\n",
        "- You can only use HMM or CRF model to complete the assignment.\n",
        "- If you apply HMM, it is allowed to use nltk.HiddenMarkovModelTagger to train the model.\n",
        "- Your code should run without errors\n"
      ],
      "metadata": {
        "id": "f3YuQNYrwHHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vietnamese Word Segmentation\n",
        "\n",
        "The smallest unit in the Vietnamese language is syllable (tiếng). A word may consist of one or multiple consecutive words. In some problems (such as extracting keywords from text), it is necessary to identify the words in the text.\n",
        "\n",
        "The input of the word tokenizer is a sentence consisting of syllables, and the output is a sentence with words segmented.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: Nam là sinh viên đại học ngành kỹ thuật\n",
        "\n",
        "Output: Nam là sinh_viên đại_học ngành kỹ_thuật\n",
        "\n",
        "The underscore symbol \"_\" is used to connect syllables that belong to the same word.\n",
        "\n",
        "There are many ways to solve the word segmentation problem. In this exercise, we will use a sequence labeling model to do the task. We use the BI labeling method to label each syllable in the sentence. The tag B-W is used to mark the beginning of a word, and the tag I-W is used to mark a syllable that is inside the same word as the previous syllable.\n",
        "\n",
        "If we can label each syllable in the input sentence, we can accurately tokenize the sentence.\n",
        "\n",
        "```\n",
        "Nam/B-W là/B-W sinh/B-W viên/I-W đại/B-W học/I-W ngành/B-W kỹ/B-W thuật/I-W\n",
        "```\n",
        "\n",
        "We can determine the words in the sentence from the above output. A sequence of syllables labeled as B-W I-W ... will form a word."
      ],
      "metadata": {
        "id": "HURfF5auqsCR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Afy_L8xEe1"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "You will use the train data in the file [train.txt](https://drive.google.com/file/d/1Y4AuWqbInOv1HNMiGPMhuntcA-n1jfWF/view?usp=share_link) to train your Vietnamese word segmentation model and evaluate the model image on the test data in the file [test.txt](https://drive.google.com/file/d/1Y57hlYLpxVCZUbVuwOa1IRCLgUiEAx0c/view?usp=share_link) extracted from the Word Segmentation VLSP 2013 dataset.\n",
        "\n",
        "You can download the file using the wget command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krTV-czHgcox"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!rm -f train.txt\n",
        "!gdown --id 1Y4AuWqbInOv1HNMiGPMhuntcA-n1jfWF\n",
        "\n",
        "!rm -f test.txt\n",
        "!gdown --id 1Y57hlYLpxVCZUbVuwOa1IRCLgUiEAx0c"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ChznIKgime"
      },
      "source": [
        "The training data contains 20000 sentences (sentences separated by a blank line) and the test data contains 2000 sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlfKOY9DDAjz",
        "outputId": "d40c894a-71c2-4892-fb6b-2e08d8247feb"
      },
      "source": [
        "!head -n 10 train.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nam\tB-W\n",
            "hồn\tB-W\n",
            "nhiên\tI-W\n",
            ":\tB-W\n",
            "\"\tB-W\n",
            "Tụi\tB-W\n",
            "tôi\tB-W\n",
            "xài\tB-W\n",
            "tiền\tB-W\n",
            "ngân\tB-W\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task Description\n",
        "\n",
        "- Your task is to build a tagging model that can predict a tag sequence (a tag sequence of B-W and I-W) for words in an input sentence.\n",
        "- You can use HMM or CRF model to complete the assignment.\n",
        "- If you apply HMM, it is allowed to use `nltk.HiddenMarkovModelTagger` to train the model.\n",
        "\n",
        "The package [seqeval](https://github.com/chakki-works/seqeval) can be used to evaluate the result of model word segmentation.\n"
      ],
      "metadata": {
        "id": "tagiE-Q9xrJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q seqeval[cpu]"
      ],
      "metadata": {
        "id": "IQwJL6mxaGCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087a387f-08a1-4cd5-a95c-ea73fba51e76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data\n"
      ],
      "metadata": {
        "id": "ZY-FrQAJZrIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load data from a file (train.txt or test.txt)\n",
        "\n",
        "    Return:\n",
        "        tagged_sentences (list): List of sentence. Each sentence is a list of tuples (word, tag)\n",
        "    \"\"\"\n",
        "    tagged_sentences = []\n",
        "    cur_sen = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line == '':\n",
        "                if len(cur_sen) != 0:\n",
        "                    tagged_sentences.append(cur_sen)\n",
        "                    cur_sen = []\n",
        "            else:\n",
        "                word, tag = line.split()\n",
        "                cur_sen.append((word, tag))\n",
        "    if len(cur_sen) != 0:\n",
        "        tagged_sentences.append(cur_sen)\n",
        "    return tagged_sentences\n",
        "\n",
        "train_data = load_data('train.txt')\n",
        "test_data = load_data('test.txt')"
      ],
      "metadata": {
        "id": "2ssCvw37Zuvi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Building a Word Segmentation model for Vietnamese (70 points)\n",
        "\n",
        "In this section, you will build a tagging model using HMM or CRF. \n",
        "\n",
        "*Hint*: Please refer to [HMM_POS_Tagger.ipynb](https://colab.research.google.com/drive/1lcTncvhlhx8KaJ_oBW6MR7cy470MMpD9#scrollTo=Ty-Qh9Jo23dS) or [CRF_POS_Tagger.ipynb](https://colab.research.google.com/drive/1SuxBmZudn4Tn3w-pBXDoRsuSkBA7RiVV) to understand how to build a tagging model with HMM and CRF.\n"
      ],
      "metadata": {
        "id": "YqnEC1SfKpF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_all_caps(word):\n",
        "    return word.upper() == word and not word.isdigit()\n",
        "\n",
        "def word2features(sentence, i):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        sentence (list): list of words [w1, w2,...,w_n]\n",
        "        i (int): index of the word\n",
        "    Return:\n",
        "        features (dict): dictionary of features\n",
        "    \"\"\"\n",
        "    word = sentence[i]\n",
        "    features = {\n",
        "        'is_first': i == 0,\n",
        "        'is_last': i == len(sentence) - 1,\n",
        "        'is_first_capital': word[0].isupper(),\n",
        "        'is_all_caps': is_all_caps(word),    # ????\n",
        "        'is_all_lower': word.lower() == word,  # ????\n",
        "        'word': word,\n",
        "        'word.lower()': word.lower(),\n",
        "        'prefix_1': word[0],\n",
        "        'prefix_2': word[:2],\n",
        "        'prefix_3': word[:3],\n",
        "        'prefix_4': word[:4],\n",
        "        'suffix_1': word[-1],\n",
        "        'suffix_2': word[-2:],\n",
        "        'suffix_3': word[-3:],\n",
        "        'suffix_4': word[-4:],\n",
        "        'prev_word': '' if i==0 else sentence[i-1].lower(),\n",
        "        'next_word': '' if i==len(sentence)-1 else sentence[i+1].lower(),\n",
        "        'has_hyphen': '-' in word,\n",
        "        'is_numeric': word.isdigit(),\n",
        "        'capitals_inside': word[1:].lower() != word[1:]    # ????\n",
        "    }\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of words [w1, w2,...,w_n]\n",
        "    \"\"\"\n",
        "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "\n",
        "def sent2labels(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"    \n",
        "    return [postag for token, postag in sentence]\n",
        "\n",
        "def untag(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"\n",
        "    return [token for token, _ in sentence]"
      ],
      "metadata": {
        "id": "by5Ac1jHpu1I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent2features( untag(train_data[0]) )[10]"
      ],
      "metadata": {
        "id": "ZtM-tOU3PckZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff93858-da91-443a-b5cc-c76577196b82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is_first': False,\n",
              " 'is_last': False,\n",
              " 'is_first_capital': False,\n",
              " 'is_all_caps': False,\n",
              " 'is_all_lower': True,\n",
              " 'word': 'hàng',\n",
              " 'word.lower()': 'hàng',\n",
              " 'prefix_1': 'h',\n",
              " 'prefix_2': 'hà',\n",
              " 'prefix_3': 'hàn',\n",
              " 'prefix_4': 'hàng',\n",
              " 'suffix_1': 'g',\n",
              " 'suffix_2': 'ng',\n",
              " 'suffix_3': 'àng',\n",
              " 'suffix_4': 'hàng',\n",
              " 'prev_word': 'ngân',\n",
              " 'next_word': 'không',\n",
              " 'has_hyphen': False,\n",
              " 'is_numeric': False,\n",
              " 'capitals_inside': False}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [sent2features(untag(s)) for s in train_data]\n",
        "y_train = [sent2labels(s) for s in train_data]\n",
        "\n",
        "X_test = [sent2features(untag(s)) for s in test_data]\n",
        "y_test = [sent2labels(s) for s in test_data]"
      ],
      "metadata": {
        "id": "pI40KRv3wfWW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-crfsuite"
      ],
      "metadata": {
        "id": "WaVXjLa7VXpJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pycrfsuite\n",
        "\n",
        "trainer = pycrfsuite.Trainer(algorithm='lbfgs', verbose=False)\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):\n",
        "    trainer.append(xseq, yseq)"
      ],
      "metadata": {
        "id": "TWEyFdUXVWhg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBh5um0xVbYf",
        "outputId": "490c841b-b303-463e-be5a-6961d8340f9e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feature.minfreq',\n",
              " 'feature.possible_states',\n",
              " 'feature.possible_transitions',\n",
              " 'c1',\n",
              " 'c2',\n",
              " 'max_iterations',\n",
              " 'num_memories',\n",
              " 'epsilon',\n",
              " 'period',\n",
              " 'delta',\n",
              " 'linesearch',\n",
              " 'max_linesearch']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.set_params({\n",
        "    'c1': 1.0,   # coefficient for L1 penalty\n",
        "    'c2': 1e-3,  # coefficient for L2 penalty\n",
        "    'max_iterations': 50,  # stop earlier\n",
        "\n",
        "    # include transitions that are possible, but not observed\n",
        "    'feature.possible_transitions': True\n",
        "})"
      ],
      "metadata": {
        "id": "86uhqzfeVd96"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train('postagger.crfsuite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb093xNXVj3W",
        "outputId": "1935e6e4-ee29-473a-90ee-674864a6fa1f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.7 s, sys: 44.7 ms, total: 22.8 s\n",
            "Wall time: 22.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9i3MmMLxrBm"
      },
      "source": [
        "## Part 2: Model evaluation (30 points)\n",
        "\n",
        "Evaluation measures:\n",
        "\n",
        "- P(recision): (Number of word models correctly split)/(Number of words in the model's output)\n",
        "- R(ecall): (Number of word models correctly split)/(Number of words in ground-truth data)\n",
        "- F1 = 2*P*R/(P+R)\n",
        "\n",
        "What you need to do is to use the model that you have trained to generate the list of tag sequences for sentences in the test data.\n",
        "\n",
        "**Complete the following function**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tag(syllables):\n",
        "    \"\"\"Return a list of tags for a list of syllables\n",
        "\n",
        "    Arguments:\n",
        "        tokens (list)\n",
        "    \n",
        "    Returns:\n",
        "        tags (list): list of tags for input tokens\n",
        "    \"\"\"\n",
        "    # TODO: Write your code here\n",
        "    # tags = ...\n",
        "    # return tags\n",
        "    tags = pycrfsuite.Tagger()\n",
        "    tags.open('postagger.crfsuite')\n",
        "    return tags\n",
        "    pass"
      ],
      "metadata": {
        "id": "oyzF69JWC5MU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the above function to get the predicted tag sequences of the model."
      ],
      "metadata": {
        "id": "tItuUdUwaepF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## First get the unlabeled sentences and corresponding tag sequences\n",
        "test_sentences = []\n",
        "test_tag_sequences = []\n",
        "\n",
        "for sen in test_data:\n",
        "    words, tags = zip(*sen)\n",
        "    assert len(words) == len(tags)\n",
        "    test_sentences.append(list(words))\n",
        "    test_tag_sequences.append(list(tags))\n",
        "\n",
        "predicted_tag_sequences = [tag(s) for s in test_sentences]"
      ],
      "metadata": {
        "id": "O-KAD_g-akdx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, just use seqeval package to calculate precision, recall and f1 score"
      ],
      "metadata": {
        "id": "XLJusAxcDoNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "print(classification_report(test_tag_sequences, predicted_tag_sequences))"
      ],
      "metadata": {
        "id": "9hBoS3uRJ-BO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "e739d72c-5e2f-4ff1-b368-6d0fb0cfe33f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-82220dd52aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tag_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_tag_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, digits, suffix, output_dict, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0mtarget_names_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0mtarget_names_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m     \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names_true\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtarget_names_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mget_entities\u001b[0;34m(seq, suffix)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0m_validate_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36m_validate_chunk\u001b[0;34m(chunk, suffix)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'E-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'S-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} seems not to be NE tag.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'pycrfsuite._pycrfsuite.Tagger' object has no attribute 'startswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3XiagurwdKr"
      },
      "source": [
        "## References\n",
        "\n",
        "- Huyen, N. T. M., Roussanaly, A., & Vinh, H. T. (2008, March). A hybrid approach to word segmentation of Vietnamese texts. In International Conference on Language and Automata Theory and Applications (pp. 240-249). Springer, Berlin, Heidelberg.\n",
        "- Nguyen, T. P., & Le, A. C. (2016, November). [A hybrid approach to Vietnamese word segmentation](https://www.researchgate.net/profile/Tuan_Phong_Nguyen/publication/311980397_A_hybrid_approach_to_Vietnamese_word_segmentation/links/5a9507e3a6fdccecff0771ff/A-hybrid-approach-to-Vietnamese-word-segmentation.pdf). In 2016 IEEE RIVF International Conference on Computing & Communication Technologies, Research, Innovation, and Vision for the Future (RIVF) (pp. 114-119). IEEE.\n",
        "- Nguyen, D. Q., Nguyen, D. Q., Vu, T., Dras, M., & Johnson, M. (2017). [A fast and accurate vietnamese word segmenter](https://arxiv.org/abs/1709.06307). arXiv preprint arXiv:1709.06307.\n",
        "- [seqeval](https://github.com/chakki-works/seqevalhttps://github.com/chakki-works/seqeval) for sequence labeling evaluation.\n",
        "\n"
      ]
    }
  ]
}