{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Student's name: Ha Trung Tin\n",
        "\n",
        "Student's ID: BI-261"
      ],
      "metadata": {
        "id": "F-n91UPRPPTX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXnI7KKa5AVF"
      },
      "source": [
        "# Programming Assignment 2: Bigram Language Models\n",
        "\n",
        "In Programming Assignment 1, we are going to implement a bigram language model and use interpolation for smoothing.\n",
        "\n",
        "The bigram probabilities will be estimated by using MLE as follows:\n",
        "\n",
        "$$\n",
        "P_{ML}(w_i|w_{i-1})=\\frac{c(w_{i-1}w_{i})}{c(w_{i-1})}\n",
        "$$\n",
        "\n",
        "Recall that, the smoothed bigram probabilities using interpolation technique is calculated as follows.\n",
        "\n",
        "Bigrams:\n",
        "\n",
        "$$\n",
        "P(w_i|w_{i-1})=\\lambda_2 \\times P_{ML}(w_i|w_{i-1})+(1-\\lambda_2)\\times P(w_i)\n",
        "$$\n",
        "\n",
        "where $P(w_i)$ is the smoothed unigram probability and calculated as follows.\n",
        "\n",
        "$$\n",
        "P(w_i)=\\lambda_1 \\times P_{ML}(w_i) + (1-\\lambda_1) \\times \\frac{1}{N}\n",
        "$$\n",
        "\n",
        "where $N$ is a large number, e.g., $N=1,000,000$.\n",
        "\n",
        "There are two parts in this programming assignment.\n",
        "\n",
        "- Training: you will estimate unigram and bigram probabilities from a raw text data file and save parameters of the language model into a file.\n",
        "- Test: you use the trained language model to calculate calculates entropy, perplexity on the test data. You will need to use interpolation to calculate smoothed probabilities in testing.\n",
        "\n",
        "Please refer the pseudo-code in slide 17, 18 of the lecture [NLP Programming Tutorial 2 -Bigram Language Models](http://www.phontron.com/slides/nlp-programming-en-02-bigramlm.pdf) to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1tTFpUuV10R"
      },
      "source": [
        "## Submission and due date\n",
        "\n",
        "**The due for the programming assignment 2 will be at 23:59 on February 19, 2023 (Hard deadline)**\n",
        "\n",
        "### How to submit\n",
        "\n",
        "- Attach notebook file (.ipynb) and submit your work to Google Class Room \n",
        "- Name your file as YourName_StudentID_Assignment1.ibynb. E.g., Nguyen_Van_A_ST099834_Assignment2.ipynb\n",
        "- Copying others' assignments is strictly prohibited.\n",
        "- Write your name and student ID into this notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxKE4cMUcH3"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use the file [wiki-en-train.word](https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word) as the training data, and [wiki-en-test.\n",
        "word](https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word) as the test data. To test our implementation quickly, we will use small data file [02-train-input.txt](https://github.com/neubig/nlptutorial/blob/master/test/02-train-input.txt). All data files are from the [nlptutorial](https://github.com/neubig/nlptutorial) by Graham Neubig.\n",
        "\n",
        "As the first step, we will download all necessary data files using `wget` command line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5JRqLFIVnI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcd5495-75ee-492c-cf78-07a80b0040b5"
      },
      "source": [
        "!rm -f wiki-en-train.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
        "    \n",
        "!rm -f wiki-en-test.word\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
        "\n",
        "!rm -f 02-train-input.txt\n",
        "!wget https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 15:57:45--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-train.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203886 (199K) [text/plain]\n",
            "Saving to: ‘wiki-en-train.word’\n",
            "\n",
            "wiki-en-train.word  100%[===================>] 199.11K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-02-18 15:57:45 (7.68 MB/s) - ‘wiki-en-train.word’ saved [203886/203886]\n",
            "\n",
            "--2023-02-18 15:57:45--  https://raw.githubusercontent.com/neubig/nlptutorial/master/data/wiki-en-test.word\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26989 (26K) [text/plain]\n",
            "Saving to: ‘wiki-en-test.word’\n",
            "\n",
            "wiki-en-test.word   100%[===================>]  26.36K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-02-18 15:57:45 (17.1 MB/s) - ‘wiki-en-test.word’ saved [26989/26989]\n",
            "\n",
            "--2023-02-18 15:57:45--  https://raw.githubusercontent.com/neubig/nlptutorial/master/test/02-train-input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12 [text/plain]\n",
            "Saving to: ‘02-train-input.txt’\n",
            "\n",
            "02-train-input.txt  100%[===================>]      12  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 15:57:45 (906 KB/s) - ‘02-train-input.txt’ saved [12/12]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0WLgNXeVt0Z"
      },
      "source": [
        "## Part 1: Estimating probabilities (60 points)\n",
        "\n",
        "What you need to do in this part is complete the function `train_bigram` to estimate unigram, bigram probabilities (using MLE method) from a text file and save probabilities to a model file.\n",
        "\n",
        "The format of the model file is as follows. Each line includes an n-gram (unigram or bigram) and its probability estimated by MLE method.\n",
        "\n",
        "```\n",
        "<s> a\t1.000000\n",
        "a\t0.250000\n",
        "a b\t1.000000\n",
        "b\t0.250000\n",
        "b c\t0.500000\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP2Tj-kZjowV"
      },
      "source": [
        "### Part 1.1: Function train_bigram()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX2a0Yetfzta"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def train_bigram(train_file, model_file):\n",
        "    \"\"\"Train trigram language model and save to model file\n",
        "    \"\"\"\n",
        "    counts = defaultdict(int)  # count the n-gram\n",
        "    context_counts = defaultdict(int)   # count the context\n",
        "    count0 = 0\n",
        "    with open(train_file) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "    \n",
        "            for i in range(1, len(words)):  # Note: starting at 1, after <s>\n",
        "                # TODO: Write code to count bigrams and their contexts\n",
        "                # YOUR CODE HERE\n",
        "                # Add bigram and bigram context\n",
        "                counts[words[i-1]+ \" \"+ words[i]] += 1             \n",
        "                context_counts[words[i-1]] += 1\n",
        "                # Add unigram and unigram context\n",
        "                counts[words[i]] += 1\n",
        "                context_counts[\"\"] += 1\n",
        "                count0 += 1\n",
        "                pass\n",
        "\n",
        "    # Save probabilities to the model file            \n",
        "    with open(model_file, 'w') as fo:\n",
        "        for ngram, count in counts.items():\n",
        "            # TODO: Write code to calculate probabilities of n-grams \n",
        "            # (unigrams and bigrams)\n",
        "            # Hint: probabilities of n-grams will be calculated by their counts\n",
        "            # divided by their context's counts.\n",
        "            # probability = counts[ngram]/context_counts[context]\n",
        "            # After calculating probabilities, we will save ngram and probability\n",
        "            # to the file in the format:\n",
        "            # ngram<tab>probability\n",
        "\n",
        "            # YOUR CODE HERE\n",
        "            ngram_split = ngram.split()\n",
        "\n",
        "            # BIGRAM\n",
        "            if len(ngram_split) > 1: \n",
        "              fo.write((ngram + \"\\t\" + str(counts[ngram]/context_counts[ngram_split[0]]))+\"\\n\")\n",
        "\n",
        "            # UNIGRAM\n",
        "            else:\n",
        "              fo.write((ngram + \"\\t\" + str(counts[ngram]/count0))+\"\\n\")\n",
        "            pass"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpbTBe8xi1rH"
      },
      "source": [
        "Let's try to train bigram model on the small data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lmBGnZWi5_R"
      },
      "source": [
        "train_bigram('02-train-input.txt', '02-train-answer.txt')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xofAVqrNjEHA"
      },
      "source": [
        "Let's see the content of the model. After completing the function `train_bigram`, you should see. The order of lines may be different.\n",
        "\n",
        "```\n",
        "</s>\t0.250000\n",
        "<s> a\t1.000000\n",
        "a\t0.250000\n",
        "a b\t1.000000\n",
        "b\t0.250000\n",
        "b c\t0.500000\n",
        "b d\t0.500000\n",
        "c\t0.125000\n",
        "c </s>\t1.000000\n",
        "d\t0.125000\n",
        "d </s>\t1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Wf9fXyjLwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab50235c-07fe-492b-f87c-921a579017de"
      },
      "source": [
        "!cat 02-train-answer.txt"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> a\t1.0\n",
            "a\t0.25\n",
            "a b\t1.0\n",
            "b\t0.25\n",
            "b c\t0.5\n",
            "c\t0.125\n",
            "c </s>\t1.0\n",
            "</s>\t0.25\n",
            "b d\t0.5\n",
            "d\t0.125\n",
            "d </s>\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpiLpxjM5z"
      },
      "source": [
        "### Part 1.2: load the model file\n",
        "\n",
        "We are going to implement the function `load_bigram_model` to load the model file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0lc9hiMkAJk"
      },
      "source": [
        "def load_bigram_model(model_file):\n",
        "    \"\"\"Load the model file\n",
        "\n",
        "    Args:\n",
        "        model_file (str): Path to the model file\n",
        "    \n",
        "    Returns:\n",
        "        probs (dict): Dictionary object that map from ngrams to their probabilities\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    with open(model_file, 'r') as f:\n",
        "        for line in f:\n",
        "            # TODO: From each line split ngram, probability\n",
        "            # and then update probs\n",
        "            \n",
        "            # YOUR CODE HERE\n",
        "            element = line.split(\"\\t\")\n",
        "            probs[element[0]] = float(element[1])\n",
        "            \n",
        "            pass\n",
        "    return probs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czcC1vOIq3YV"
      },
      "source": [
        "Let's test the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3pTilwdq5lP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303d23cc-37b7-4214-95ac-356eb81b811f"
      },
      "source": [
        "probs = load_bigram_model('02-train-answer.txt')\n",
        "probs"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<s> a': 1.0,\n",
              " 'a': 1.0,\n",
              " 'a b': 1.0,\n",
              " 'b': 1.0,\n",
              " 'b c': 0.5,\n",
              " 'c': 1.0,\n",
              " 'c </s>': 1.0,\n",
              " '</s>': 1.0,\n",
              " 'b d': 0.5,\n",
              " 'd': 1.0,\n",
              " 'd </s>': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBpjGSG8ky7u"
      },
      "source": [
        "## Part 2: Evaluating Bigram Language Model (40 points)\n",
        "\n",
        "In this part, we will evaluate the bigram language model on the test set. We will use linear interpolation as the smoothing technique.\n",
        "\n",
        "What we need to do is to complete the function  `test_bigram` as follows. The function will return perplexity on the test data.\n",
        "\n",
        "Recall that, the smoothed bigram probabilities using interpolation technique is calculated as follows.\n",
        "\n",
        "Bigrams:\n",
        "\n",
        "$$\n",
        "P(w_i|w_{i-1})=\\lambda_2 \\times P_{ML}(w_i|w_{i-1})+(1-\\lambda_2)\\times P(w_i)\n",
        "$$\n",
        "\n",
        "where $P(w_i)$ is the smoothed unigram probability and calculated as follows.\n",
        "\n",
        "$$\n",
        "P(w_i)=\\lambda_1 \\times P_{ML}(w_i) + (1-\\lambda_1) \\times \\frac{1}{N}\n",
        "$$\n",
        "\n",
        "where $N$ is a large number, e.g., $N=1,000,000$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLb5kGJilgrG"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def test_bigram(test_file, model_file, lambda2=0.95, lambda1=0.95, N=1000000):\n",
        "    W = 0 # Total word count\n",
        "    H = 0\n",
        "    probs = load_bigram_model(model_file)\n",
        "    with open(test_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                continue\n",
        "            words = line.split()\n",
        "            words.append('</s>')\n",
        "            words.insert(0, '<s>')\n",
        "            for i in range(1, len(words)):  # Note: starting at 1, after <s>\n",
        "                # TODO: Write code to calculate smoothed unigram probabilties\n",
        "                # and smoothed bigram probabilities\n",
        "                # You should use calculate p1 as smoothed unigram probability\n",
        "                # and p2 as smoothed bigram probability\n",
        "                p1 = None\n",
        "                p2 = None\n",
        "\n",
        "                # YOUR CODE HERE\n",
        "                # If the unigram exists in the dictionary\n",
        "                if (probs.get(words[i])):\n",
        "                  p1 = lambda1 * probs.get(words[i]) + (1-lambda1) * (1/N)\n",
        "                else: # We assume the value of probs.get(words[i]) to be 0 --> No unigram exists\n",
        "                  p1 = lambda1 * 0 + (1-lambda1) * (1/N) \n",
        "                \n",
        "                # If the bigram exists in the dictionary\n",
        "                if (probs.get(words[i-1] + \" \" + words[i])):\n",
        "                  p2 = lambda2 * probs.get(words[i-1] + \" \" + words[i])+ (1-lambda2) * p1\n",
        "                else: # We assume the value of probs.get(words[i-1] + \" \" + words[i]) to be 0 --> No bigram exists\n",
        "                  p2 = lambda2 * 0 + (1-lambda2) * p1  \n",
        "                # END OF YOUR CODE\n",
        "\n",
        "                W += 1  # Count the words\n",
        "                H += -math.log2(p2) # We use logarithm to avoid underflow\n",
        "    H = H/W\n",
        "    P = 2**H\n",
        "    \n",
        "    print(\"Entropy: {}\".format(H))\n",
        "    print(\"Perplexity: {}\".format(P))\n",
        "\n",
        "    return P"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To39tEQ0pAH6"
      },
      "source": [
        "Now let's calculate on the Wikipedia data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHAE96uvpQPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d120bd85-dcc3-4ea2-c8b7-83c4490178bf"
      },
      "source": [
        "train_bigram('wiki-en-train.word', 'bigram_model.txt')\n",
        "test_bigram('wiki-en-test.word', 'bigram_model.txt')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy: 11.284859117885485\n",
            "Perplexity: 2495.0605603552463\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2495.0605603552463"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQcvtIM1wRh-"
      },
      "source": [
        "You should get entropy value about 11.28."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiyicE5hpUyb"
      },
      "source": []
    }
  ]
}